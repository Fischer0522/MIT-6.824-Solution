---
title: 6.824 Lab1通关指南
date: 2023-03-27T16:19:14Z
lastmod: 2023-03-27T20:44:21Z
categories: [Distributed System,MIT 6.824]
---

# 6.824 Lab1通关指南

Lab1在当时做的时候边做边写过一些思路分析，当时主要是一点点的分析论文，主要针对于各个函数怎么实现的，但是对于整体的描绘并不是很清楚，因此接着复习项目的机会重新写一份通关指南，希望能对MapReduce能有一个整体上的视角和分析一下个别需要注意的细节。

## 整体架构/处理流程

对于MapReduce的整体架构和运行方式，下面这两张图已经解释的很清楚。

简单来说，节点分为两类：

* 一类是Master节点，和client进行对接，管理当前的任务，进行任务的分配（管理worker的状态）
* 一类是Worker节点，worker节点并不需要进行细分为MapWorker和ReduceWorker，只需根据传来的任务类型进行处理即可。

**处理流程**

1. 首先master节点会接收到一个client传来的任务，如果为一个整体，则需要对其进行细分，分为不同的Map Task，用于指派给Worker
2. MapTask创建完之后，进入Map Phase，此时只分配Map Task，通过RPC的方式将发送Task，此时如果Task数量足够多的话，所有的Worker都有可能接到Map Task
3. Worker进行Task的处理，当处理完成之后，将处理结果写成一个中间文件的形式，之后告知Master
4. Master将完成的任务从Task队列当中移除，如果所有的Map Task全部完成，进行一个Shuffle过程，则转换到Reduce Phase
5. Master向Worker分配 Reduce Task，处理过程和Map Task差不多
6. 所有Reduce Task均完成，一次请求结束。

‍

![](https://pic-bed-1309931445.cos.ap-nanjing.myqcloud.com/blog/image-20230114215900-2gtlr2g.png)

![](https://pic-bed-1309931445.cos.ap-nanjing.myqcloud.com/blog/image-20230114230911-vujy1up.png)

## 实现

### Master

对于Master，我的主要设计思路就是如果所有的Worker全部宕机了，那么Master再怎么去做记录，做分配也无济于事。因此不如让任务申请的主动权交给Worker，即Worker自身处于一个循环当中，不断的向Master去请求任务，如果当前有任务可分配，那么就分配任务给Worker，否则则让Worker休眠一阵，而如果在Reduce Phase并且所有的任务全部分配完成，那么则证明此次处理已经完成，Worker再请求时直接返回一个信号让Worker进程结束即可。

因此Worker并不需要维护任何和Worker相关的信息，只需要维护集群的一些配置信息，以及当前的Task队列和当前整个MapReduce处于的状态，最后再定义一个二位数组用于存储中间文件即可，Map结构体如下：

```go
type Coordinator struct {
	// Your definitions here.
	assignedId     int
	files          []string
	phase          int // 0 for map phase 1 for reduce phase, 2 means all works finished
	nMap           int
	nReduce        int
	mapTasks       []Task
	mapTaskLeft    int
	reduceTasks    []Task
	reduceTaskLeft int
	muTasks        sync.Mutex

	interFiles [][]string
}
```

由于Map在任务分配上处于被动状态，但是具体分配什么任务有Master决定，因此需要提供一个`AssignTask`​的RPC供Worker调用即可，`AssignTask`​只需要根据当前处于的阶段进行分配任务即可。而对于一个任务，主要有三个状态量：

* isTimeout：该任务是否超时，如果超时则需要对其进行重新分配
* isAssigned：该任务是否被分配，但是可能会因为超时而对其进行重新分配。
* isFinished：任务是否结束，如果结束则彻底不再对其分配。

而一个任务在调用AssignTask之前只能处于以下六种状态，自左向右分别为 isTimeout，isAssigned，isFinished：

* F F F：初始状态，还未分配
* T F F:之前分配出去现已经超时，需要后续重新分配
* F T F：分配了但是还未完成，等待完成或者超时
* F T T：分配了并且完成，在响应结果时移除队列或者分配任务时跳过

因此对于分配出去的任务计算时间，判断是否超时，如果超时则isAssigned设置为false，之后对其进行重新分配，之后遍历所有的Task，找一个还未分配的Task分配给Worker。

此外还需要提供一个RPC，当Worker完成之后供其调用，对已经完成的任务进行标记，或者直接将其移出队列。并且存储Worker返回的中间文件名称，用于后续的Shuffle过程。

### Worker

Woker我选择的是实现为无状态的形式，即并不需要存储任何相关的信息，只需要不断的想master去索取任务，处理任务即可，worker处理任务所依赖的全部信息都通过Master来获取，其实也没有什么信息，只需要确认集群的配置m r的数量，依次生成中间文件，以及需要处理的文件，因此RPC如下：

```go
type Response struct {
	FileName []string
	TaskType int
	NMap     int
	NReduce  int
	Id       int
	Index    int
}
```

其中除了Index都无需多做解释，Index留在后面故障恢复时再进行解释。

因此Worker本身就是处于一个循环当中，根据索要到的Task类型调用不同的函数进行处理即可，框架十分甚至九分的简单，

```go
func Worker(mapf func(string, string) []KeyValue,
	reducef func(string, []string) string) {
	count := 0 // for debug
	for {
		res := AskForTask()

		fmt.Printf("---------------------------------handing the %d work----------------------------\n", count)
		count++
		switch res.TaskType {
		case mapTask:
			doMapTask(mapf, res)
		case reduceTask:
			doReduceTask(reducef, res)
		case waitTask:
			fmt.Println("waiting here...")
			time.Sleep(1 * time.Second)
		case exitTask:
			return
		default:
			panic(fmt.Sprintf("unknown task type"))
		}

	}

}
```

MapTask处理完成的返回结果也比较简单，只需要告知master哪个处理完成，以及对应的中间文件即可：

```go
type MapResult struct {
	TaskIndex int
	InterFile []string
	TaskType  int
}
```

而Reduce Task的处理结果就更简单了，告知类型和Index即可，因此我直接复用了MapResult的结构。

### shuffle

一个Map Task所包含的为一个完整任务的一部分，但是里面会有很多个类，而一个Reduce Task同样负责一个完整任务的一部分，但是里面却应当只有一类。也可以理解成Map为对任务进行水平切分，Task为垂直切分，以单词统计为例，一个Map Task有各类单词，但是一个Reduce Task当中只有特定种类的单词，如果A开头的和B开头的全部归于RT1，C D开头的全归于RT2，因此就需要一个Shuffle过程

处理Map Task时对结果进行一个哈希，按照单词类型进行分类，输出到nReduce个中间文件当中，nMap个Map Task就会产生nMap * nReduce个中间文件，即为一个nM * nR的矩阵，shuffle的过程就是对其进行一个转置，之后按列创建reduce Task

```go
func shuffle(interfiles [][]string) []Task {
	// shuffle

	tasks := make([]Task, 0)
	for i := 0; i < len(interfiles[0]); i++ {
		temp := make([]string, 0, len(interfiles))
		for j := 0; j < len(interfiles); j++ {
			temp = append(temp, interfiles[j][i])
		}
		task := createReduceTask(temp)
		tasks = append(tasks, task)
	}
	return tasks
}
```

### 故障恢复

对于故障恢复，主要分为两部分，一部分是如何检测出是否发生故障，而另一部分则是对于故障脏数据，应当如何对其进行清理

#### 故障检测

故障检测依赖于超时，如果一个任务分配出去超过十秒还没完完成，那么即可认定对应的work发生了宕机，因此此时需要撤销对其任务的分配，后续再分配个其他的节点。这里对于超时的检测，参考Redis对于过期Key的清理，一共有两种实现方式：

* 一是设置一个后台线程去不断的轮询所有的task，一旦超时则撤销对其的分配
* 而另一种则是在需要寻找一个任务进行分配时遍历所有的Task，标记超时的Task

个人认为第二种方案更好一些，Redis开启一个后台进程去不断扫描并标记所有的key的原因是即时的清理掉Key，释放内存，但是Task并不存在内存层面上的考虑，而且就算超时了如果没有worker来请求任务，标记超时也没有什么意义。

#### 垃圾数据

若一个节点在写到一半时发生宕机，则会留下垃圾数据在本地，从而影响后续的shuffle以及最终数据的生成。

最开始我想的一个办法是，在每次分配请求时，携带一个自增的index，之后生成中间文件时使用该index进行生成，同时worker将该index再返回给master，参与shuffle也只有这些文件，即可避免到垃圾数据对于shuffle的影响，在reduce阶段也可以采用同样的方法指定index来聚合出最终结果。

理论上应该是没有问题的，但是Lab的测试最终是使用*通配符来匹配结果的，这样就导致无法指定文件参与reduce后的最终聚合。因此最终还是采用了临时文件的方案，即现对一个临时文件进行写入，当最后确定没有问题时再对其进行重命名，命名为规定格式，这样即可保证垃圾数据不会对最终的聚合产生影响。

‍

#### 

‍
